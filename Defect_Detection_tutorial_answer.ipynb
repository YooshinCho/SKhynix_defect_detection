{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Defect_Detection_tutorial_answer",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YooshinCho/SKhynix_defect_detection/blob/master/Defect_Detection_tutorial_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MhoQ0WE77laV"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "_ckMIh7O7s6D",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "vasWnqRgy1H4",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# 첫 번째 신경망 훈련하기: 기초적인 분류 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/basic_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />TensorFlow.org에서 보기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ko/tutorials/keras/basic_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩(Colab)에서 실행하기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ko/tutorials/keras/basic_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />깃허브(GitHub) 소스 보기</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJjNjjy3Zbz0",
        "colab_type": "text"
      },
      "source": [
        "이번 튜토리얼은 우리가 딥러닝을 통해서 불량 검출을 하였을 때, 모델이 실제로 잘 불량을 검출하고 있는지 확인하는 방법을 배우게 될 것입니다. 불량 검출은 환경에 따라서 여러 가지 어려운 조건이 붙을 수 도 있지만, 간단하게는 2 class classification이라고 볼 수 있습니다. 하지만, 실제로 잘 불량이 검출되는지를 확인하기 위해서 test accuracy만을 이용하기 보다 CAM(class activation map)이라는 방법을 사용하려고 합니다. CAM은 Bolei Zhou의 Learning Deep Features for Discriminative Localization 논문에서 제시된 방법입니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CAM은 모델에 input image에 대해서 어떤 특정 class로 분류를 할 때 어떤 부분에 activation을 많이 하고 있는지를 visualization 해주는 기법입니다. 이를 위해서 상당히 간단하고 직관적이지만 효과적인 모델 구조를 가지고 있습니다. Feature map을 global average pooling한 다음 fully connectd layer를 통과시켜 최종 아웃풋을 뽑아내는 방법인대요, 이를 통해서 각 feature map이 얼마나 기여를 하고 있는지 fully connected layer의 weight를 통해서 알 수 있는 것입니다.\n",
        "\n",
        "우선 오늘의 튜토리얼의 순서는 다음과 같습니다.\n",
        "\n",
        "1. 패션 mnist 데어터셋 임포트\n",
        "\n",
        "2. 데이터 전처리\n",
        "\n",
        "3. 모델 구성\n",
        "\n",
        "4. 모델 훈련 및 평가\n",
        "\n",
        "5. CAM 영상 확인\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dzLKpmZICaWN",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras를 임포트합니다\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 헬퍼(helper) 라이브러리를 임포트합니다\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## 패션 MNIST 데이터셋 임포트하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋을 사용하겠습니다. 이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 옷 품목을 나타냅니다:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>그림 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "패션 MNIST는 컴퓨터 비전 분야의 \"Hello, World\" 프로그램격인 고전 [MNIST](http://yann.lecun.com/exdb/mnist/) 데이터셋을 대신해서 자주 사용됩니다. MNIST 데이터셋은 손글씨 숫자(0, 1, 2 등)의 이미지로 이루어져 있습니다. 여기서 사용하려는 옷 이미지와 동일한 포맷입니다.\n",
        "\n",
        "패션 MNIST는 일반적인 MNIST 보다 조금 더 어려운 문제이고 다양한 예제를 만들기 위해 선택했습니다. 두 데이터셋은 비교적 작기 때문에 알고리즘의 작동 여부를 확인하기 위해 사용되곤 합니다. 코드를 테스트하고 디버깅하는 용도로 좋습니다.\n",
        "\n",
        "네트워크를 훈련하는데 60,000개의 이미지를 사용합니다. 그다음 네트워크가 얼마나 정확하게 이미지를 분류하는지 10,000개의 이미지로 평가하겠습니다. 패션 MNIST 데이터셋은 텐서플로에서 바로 임포트하여 적재할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MqDQO0KCaWS",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S0LYQA7xNbS",
        "colab_type": "text"
      },
      "source": [
        "뒤에서 쓰인 hyper-parameter입니다. \n",
        "\n",
        "1. BATCH_SIZE는 256, EPOCHS는 10 으로 지정해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9FDsUlxCaWW"
      },
      "source": [
        "load_data() 함수를 호출하면 네 개의 넘파이(NumPy) 배열이 반환됩니다:\n",
        "\n",
        "* `train_images`와 `train_labels` 배열은 모델 학습에 사용되는 *훈련 세트*입니다.\n",
        "* `test_images`와 `test_labels` 배열은 모델 테스트에 사용되는 *테스트 세트*입니다.\n",
        "\n",
        "이미지는 28x28 크기의 넘파이 배열이고 픽셀 값은 0과 255 사이입니다. *레이블*(label)은 0에서 9까지의 정수 배열입니다. 이 값은 이미지에 있는 옷의 *클래스*(class)를 나타냅니다:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>레이블</th>\n",
        "    <th>클래스</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "각 이미지는 하나의 레이블에 매핑되어 있습니다. 데이터셋에 *클래스 이름*이 들어있지 않기 때문에 나중에 이미지를 출력할 때 사용하기 위해 별도의 변수를 만들어 저장합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjnLH5S2CaWx",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "네트워크를 훈련하기 전에 데이터를 전처리해야 합니다. 훈련 세트에 있는 첫 번째 이미지를 보면 픽셀 값의 범위를 확인하고 적당한 값으로 바꾸어줘어야 합니다.  우선 import 한 데이터셋의 픽셀값이 어떤 범위를 가지고 있는지 최댓값과 최솟값을 확인해주세요. \n",
        "([ ] 부분에 괄호를 지우고 알맞은 코드를 적어주세요)\n",
        "\n",
        "1. 최댓값, 최솟값 확인 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m4VEw8Ud9Quh",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "print(\"max pixel value: \", train_images.max()) #1\n",
        "print(\"min pixel value: \", train_images.min()) #2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "신경망 모델에 주입하기 전에 이 값의 범위를 0~1 사이로 조정하겠습니다. 이렇게 하려면 255로 나누어야 합니다. *훈련 세트*와 *테스트 세트*를 동일한 방식으로 전처리하는 것이 중요합니다. 그리고 convolution filter를 이용할 것이기 때문에 뒤에 채널 수 1을 만들어주어야 합니다.\n",
        "\n",
        "1. 픽셀 값의 범위를 0~1로 조정합니다.\n",
        "\n",
        "2. 데이터의 shape를 (data_size, height, weight, channel)로 맞춰줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bW5WzIPlCaWv",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255.0   \n",
        "test_images = test_images / 255.0 \n",
        "\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0gynFDGombo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Defected_class = 1\n",
        "\n",
        "train_labels_defect = train_labels /1\n",
        "for i in range(0, train_labels_defect.shape[0]):\n",
        "    if train_labels[i] == Defected_class:\n",
        "        train_labels_defect[i] = 0\n",
        "    else:\n",
        "        train_labels_defect[i] = 1\n",
        "        \n",
        "test_labels_defect = test_labels /1\n",
        "for i in range(0, test_labels_defect.shape[0]):\n",
        "    if test_labels[i] == Defected_class:\n",
        "        test_labels_defect[i] = 0\n",
        "    else:\n",
        "        test_labels_defect[i] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ee638AlnCaWz"
      },
      "source": [
        "*훈련 세트*에서 처음 25개 이미지와 그 아래 클래스 이름을 출력해 보죠. 데이터 포맷이 올바른지 확인하고 네트워크 구성과 훈련할 준비를 마칩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZTImqg_CaW1",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i,:,:,0], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## 모델 구성\n",
        "\n",
        "신경망 모델을 만들려면 모델의 층을 구성한 다음 모델을 컴파일합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### 층 설정\n",
        "\n",
        "신경망의 기본 구성 요소는 *층*(layer)입니다. 층은 주입된 데이터에서 표현을 추출합니다. 아마도 문제를 해결하는데 더 의미있는 표현이 추출될 것입니다.\n",
        "\n",
        "대부분 딥러닝은 간단한 층을 연결하여 구성됩니다. 'keras.layers.Dense;,  'keras.layers.Conv2D'와 같은 층들의 가중치(parameter)는 훈련하는 동안 학습됩니다.\n",
        "\n",
        "아래의 설명과 같은 구조의 모델을 정의해주세요.\n",
        "\n",
        "\n",
        "1. input의 크기는 28*28*1 입니다.\n",
        "\n",
        "2. Max pooling을 지날 때 height, weight가 절반이 됩니다. kernel의 크기는 2,2 입니다.\n",
        "\n",
        "3. 모든 conv의 kernel size는 3,3 이고, stride는 1,1, 이고 activation은 relu 입니다.\n",
        "\n",
        "4. layer의 순서를 다음 리스트를 통해 적을텐데 숫자는 conv layer를 의미하고 숫자는 해당 conv layer의 output channel의 숫자입니다. M은 max pooling을 의미합니다.\n",
        "\n",
        "#[32, M, 64,  M, 128, 128]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rf4jfJVivLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_CAM_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same',\n",
        "                                     input_shape=[28, 28, 1],activation = 'relu'))\n",
        "    \n",
        "    model.add(keras.layers.MaxPooling2D((2,2),strides = (2,2)))\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same',\n",
        "                                     input_shape=[14, 14, 32],activation = 'relu'))\n",
        "    \n",
        "    model.add(keras.layers.MaxPooling2D((2,2),strides = (2,2)))\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same',\n",
        "                                     input_shape=[7, 7, 64],activation = 'relu'))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same',\n",
        "                                     input_shape=[7, 7, 128],activation = 'relu', name = 'final_conv'))\n",
        "    \n",
        "    model.add(keras.layers.GlobalAvgPool2D())\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(10))\n",
        "    model.add(keras.layers.Softmax())\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NsMmc4K3JT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = make_CAM_model()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "### 모델 컴파일\n",
        "\n",
        "모델을 훈련하기 전에 필요한 몇 가지 설정이 모델 *컴파일* 단계에서 추가됩니다:\n",
        "\n",
        "* *손실 함수*(Loss function)-훈련 하는 동안 모델의 오차를 측정합니다. 모델의 학습이 올바른 방향으로 향하도록 이 함수를 최소화해야 합니다.\n",
        "* *옵티마이저*(Optimizer)-데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정합니다.\n",
        "* *지표*(Metrics)-훈련 단계와 테스트 단계를 모니터링하기 위해 사용합니다. 다음 예에서는 올바르게 분류된 이미지의 비율인 *정확도*를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lhan11blCaW7",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## 모델 훈련\n",
        "\n",
        "신경망 모델을 훈련하는 단계는 다음과 같습니다:\n",
        "\n",
        "1. 훈련 데이터를 모델에 주입합니다-이 예에서는 `train_images`와 `train_labels` 배열입니다.\n",
        "2. 모델이 이미지와 레이블을 매핑하는 방법을 배웁니다.\n",
        "3. 테스트 세트에 대한 모델의 예측을 만듭니다-이 예에서는 `test_images` 배열입니다. 이 예측이 `test_labels` 배열의 레이블과 맞는지 확인합니다.\n",
        "\n",
        "훈련을 시작하기 위해 `model.fit` 메서드를 호출하면 모델이 훈련 데이터를 학습합니다:\n",
        "\n",
        "1. 넣어주어야 할  argument로는 train image, train label, 총 epoch 수. batch size가 있습니다.  괄호를 지우고 알맞은 코드를 넣어주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xvwvpA64CaW_",
        "colab": {}
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=EPOCHS, batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oEw4bZgGCaXB"
      },
      "source": [
        "## 정확도 평가\n",
        "\n",
        "그다음 테스트 세트에서 모델의 성능을 비교합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VflXLEeECaXC",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('테스트 정확도:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INV9hpMP-45X",
        "colab_type": "text"
      },
      "source": [
        "##CAM 영상 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfVfRX8TrHjR",
        "colab_type": "text"
      },
      "source": [
        "이제 학습한 모델이 제대로 판단을 하고 있는지를 CAM을 그려보면서 확인할 것입니다. 필요한 것은 우선 가장 마지막 convolution filter의 output feature map이 필요하고, 저희가 원하는 class와 연결되는 fully connected layer의 weight가 필요합니다.\n",
        "따라서 우선 'get_output_layer'라는 함수를 통해서 저희가 지정해준 'final_conv'를 찾아줍니다.\n",
        "\n",
        "1. 반칸에 알맞는 dict를 써주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSaj991V9Nsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_output_layer(model, layer_name):\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer = layer_dict[layer_name]\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BMi7Rw8tWH3",
        "colab_type": "text"
      },
      "source": [
        "이제 'visualize_CAM' 함수를 채워보도록 하겠습니다. CAM 이미지를 만들기 위해서 가장 prediction이 높게 나온 class의 activation map을 만들어보도록 하겠습니다.\n",
        "\n",
        "1. 마지막 fully connected layer의 weight가 필요함으로 마지막 fully connected layer에 접근하여 weight 값을 불러옵니다. (hint: model.layers[-1].get_weights()[0] 을 이용하면 가장 마지막 layers, 즉 softmax layer의 weight에 접근하게 됩니다.)\n",
        "\n",
        "2. 위에서 정의한 'get_output_layer'와 저희가 정의해준 layer의 name을 통해서 마지막 convolution layer를 찾습니다. (hint : final_conv)\n",
        "\n",
        "3. 우선 zero로 initialized 된 공백 이미지를 만듭니다. (hint : conv_output과 h,w가 같아야합니다.)\n",
        "\n",
        "4. Fully connected layer의 weight로 가중치를 준 feature map을 공백에 더해줍니다.\n",
        "\n",
        "5. feature map의 이미지가 원본 이미지와 크기가 다르므로 사이즈를 resize 시켜줍니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCaRta4r9Pzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_CAM(model, img):\n",
        "    _, width, height, _ = img.shape\n",
        "\n",
        "    class_weights = model.layers[-2].get_weights()[0]\n",
        "    \n",
        "    final_conv_layer = get_output_layer(model, 'final_conv')\n",
        "    get_output = keras.backend.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
        "    [conv_outputs, predictions] = get_output([img])\n",
        "    conv_outputs = conv_outputs[0, :, :, :]\n",
        "    predictions = np.argmax(predictions)\n",
        "\n",
        "    \n",
        "    #Create the class activation map.\n",
        "    cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
        "    for i, w in enumerate(class_weights[:, np.argmax(predictions)]):\n",
        "            cam += w * conv_outputs[ :, :, i]\n",
        "            \n",
        "    print(\"predictions\", class_names[predictions])\n",
        "    cam = cam / np.max(cam)\n",
        "    cam = cv2.resize(cam, (28,28))\n",
        "    \n",
        "    cam[np.where(cam < 0.2)] = 0\n",
        "    \n",
        "    img = img[0,:,:,0]\n",
        "    img_cam = cam + img\n",
        "    \n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(5,5,1)\n",
        "    plt.imshow(cam, cmap=plt.cm.binary)\n",
        "    plt.subplot(5,5,2)\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    plt.subplot(5,5,3)\n",
        "    plt.imshow(img_cam, cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaprIKjlvu4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = train_images[0]\n",
        "img = np.expand_dims(img,0)\n",
        "visualize_CAM(model, img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS1lk36lkSU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_grad_CAM(model, img):\n",
        "    \n",
        "    _, width, height, _ = img.shape\n",
        "    class_weights = model.layers[-2].get_weights()[0]\n",
        "    final_conv_layer = get_output_layer(model, 'final_conv')\n",
        "    \n",
        "    \n",
        "    with tf.GradientTape() as g:\n",
        "        g.watch(final_conv_layer.output) \n",
        "        get_output = keras.backend.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
        "        \n",
        "    print(final_conv_layer.output)\n",
        "    \n",
        "    [conv_outputs, predictions] = get_output([img])\n",
        "    conv_grad = g.gradient(model.layers[-1].output, model.layers[0].input)    \n",
        "    \n",
        "    print(conv_grad)\n",
        "    predictions = np.argmax(predictions)\n",
        "    \n",
        "    \n",
        "    conv_outputs = conv_outputs[0, :, :, :]\n",
        "    \n",
        "    #Create the class activation map.\n",
        "    cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
        "    for i, w in enumerate(class_weights[:, np.argmax(predictions)]):\n",
        "            cam += w * conv_outputs[ :, :, i]\n",
        "    print(\"predictions\", np.argmax(predictions))\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / np.max(cam)\n",
        "    cam = cv2.resize(cam, (28,28))\n",
        "\n",
        "    \n",
        "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    \n",
        "    #cam[np.where(cam < 0.2)] = 0\n",
        "    \n",
        "    print(cam.shape)\n",
        "    print(img.shape)\n",
        "    img = img[0,:,:,0]\n",
        "    img = cam + img\n",
        "    \n",
        "    plt.imshow(img)\n",
        "img = train_images[2]\n",
        "img = np.expand_dims(img,0)\n",
        "\n",
        "visualize_grad_CAM(model, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yWfgsmVXCaXG"
      },
      "source": [
        ""
      ]
    }
  ]
}